# -*- coding: utf-8 -*-
"""
Created on Thu Jan 24 19:21:45 2019

@author: Atharva Kulkarni
"""
#%%
#importing all prerequisites
import keras
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.optimizers import SGD,RMSprop,adam,Adadelta
from keras.utils import np_utils
from sklearn.model_selection import train_test_split
from keras.layers.normalization import BatchNormalization
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import os
import theano
from PIL import Image
# SKLEARN

from sklearn.utils import shuffle
#from sklearn import cross_validation

from matplotlib.image import imread

#%%
#setting up the image dataset

# input image dimensions
img_rows, img_cols = 128, 128

# number of channels
img_channels = 1

##%%
#  data

path1 = './Data/Test_Data'    #path of folder of images    
path2 = './Data/Test_Data_Resized'  #path of folder to save resized images    

listing = os.listdir(path1) 
num_samples= len(listing)
print (num_samples)

#resizing all images to 128*128 and converting to grayscale
for file in listing:
    im = Image.open(path1 + '\\' + file)   
    img = im.resize((img_rows,img_cols))
    gray = img.convert('L')
                #need to do some more processing here           
    gray.save(path2 +'\\' +  file, "JPEG")

imlist = os.listdir(path2)
immatrix = []


for abc in imlist:
    image = imread(path2 + '\\' + abc)
    immatrix.append(image)

m,n = image.shape[:] # get the size of the images
imnbr = len(immatrix) # get the number of images
num_samples=len(immatrix)              
label=np.ones((num_samples,),dtype = int)
label[0:329]=0 #images till index 328 are face 1, rest are random images
label[329:]=1


data,Label = shuffle(immatrix,label, random_state=2)
train_data = [data,Label]

print (train_data[0].shape)
print (train_data[1].shape)

#%%
#defining model parameters

#batch_size to train
batch_size = 32
# number of output classes
nb_classes = 2
# number of epochs to train
nb_epoch = 20


# number of convolutional filters to use
nb_filters = 32
# size of pooling area for max pooling
nb_pool = 2
# convolution kernel size
nb_conv = 3

#%%
(X, y) = (train_data[0],train_data[1])


# STEP 1: split X and y into training and testing sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)
X_train=np.array(X_train)
X_test=np.array(X_test)


X_train = X_train.reshape(X_train.shape[0],  img_rows, img_cols,1)
X_test = X_test.reshape(X_test.shape[0],  img_rows, img_cols,1 )

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

X_train /= 255
X_test /= 255

print('X_train shape:', X_train.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)


#%%
# defining the model  (has to be revised)

class An_xyz():
    global xyz
    batch_size=32
    xyz = Sequential()
# Step 1 - Convolution
    xyz.add(Convolution2D(nb_filters, nb_conv, nb_conv,
                        border_mode='valid',
                        input_shape=(img_rows, img_cols,1)))
# Batch noormalization
    xyz.add(BatchNormalization())
# Step 2 - Pooling
    xyz.add(MaxPooling2D(pool_size = (nb_pool, nb_pool)))
# Adding a second convolutional layer
    xyz.add(Convolution2D(48 , (3, 3), activation = 'relu'))
    xyz.add(MaxPooling2D(pool_size = (nb_pool, nb_pool)))
#Adding third convolutional layer
    xyz.add(Convolution2D(32, (3, 3), activation = 'relu'))
#    xyz.add(BatchNormalization())
    xyz.add(MaxPooling2D(pool_size = (nb_pool, nb_pool)))
# Step 3 - Flattening
    xyz.add(Flatten())
# Step 4 - Full connection
    xyz.add(Dense(units = 2, activation = 'relu'))
    xyz.add(Dense(units = 2, activation = 'softmax'))
# Compiling the CNN
    xyz.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
#%%
#fitting the model

hist = xyz.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, validation_split=0.2)
xyz.summary()
#%%
# visualizing losses and accuracy

train_loss=hist.history['loss']
val_loss=hist.history['val_loss']
train_acc=hist.history['acc']
val_acc=hist.history['val_acc']
xc=range(nb_epoch)

plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss)
plt.plot(xc,val_loss)
plt.xlabel('num of Epochs')
plt.ylabel('loss')
plt.title('train_loss vs val_loss')
plt.grid(True)
plt.legend(['train','val'])

plt.style.use(['classic'])

plt.figure(2,figsize=(7,5))
plt.plot(xc,train_acc)
plt.plot(xc,val_acc)
plt.xlabel('num of Epochs')
plt.ylabel('accuracy')
plt.title('train_acc vs val_acc')
plt.grid(True)
plt.legend(['train','val'],loc=4)
#print plt.style.available # use bmh, classic,ggplot for big pictures
plt.style.use(['classic'])
